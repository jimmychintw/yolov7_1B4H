#!/usr/bin/env python3
"""
å°æ¯”è¨“ç·´çµæœï¼šBaseline vs MultiHead
åˆ†æè¨“ç·´æ—¥èªŒã€mAP æ€§èƒ½ã€é¡åˆ¥è¡¨ç¾ç­‰
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import yaml
import argparse
from glob import glob

class TrainingResultsComparator:
    """è¨“ç·´çµæœå°æ¯”å™¨"""
    
    def __init__(self, baseline_path, multihead_path):
        """
        åˆå§‹åŒ–
        
        Args:
            baseline_path: Baseline è¨“ç·´çµæœè·¯å¾‘ (runs/feasibility/baseline_be_optimized)
            multihead_path: MultiHead è¨“ç·´çµæœè·¯å¾‘ (runs/multihead/yolov7_tiny_1b4h_320)
        """
        self.baseline_path = Path(baseline_path)
        self.multihead_path = Path(multihead_path)
        
        # è¨­ç½®ç¹ªåœ–é¢¨æ ¼
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (12, 8)
        plt.rcParams['font.size'] = 10
    
    def compare_training_curves(self):
        """å°æ¯”è¨“ç·´æ›²ç·š"""
        print("\n" + "="*70)
        print("1. è¨“ç·´æ›²ç·šå°æ¯”")
        print("="*70)
        
        # è®€å–çµæœæ–‡ä»¶
        baseline_results = self._read_results(self.baseline_path / 'results.txt')
        multihead_results = self._read_results(self.multihead_path / 'results.txt')
        
        if baseline_results is None or multihead_results is None:
            print("âš ï¸ ç„¡æ³•è®€å–è¨“ç·´çµæœæ–‡ä»¶")
            return None
        
        # å‰µå»ºå­åœ–
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # å®šç¾©è¦ç¹ªè£½çš„æŒ‡æ¨™
        metrics = [
            ('train/box_loss', 'Box Loss (Train)', axes[0, 0]),
            ('train/obj_loss', 'Object Loss (Train)', axes[0, 1]),
            ('train/cls_loss', 'Class Loss (Train)', axes[0, 2]),
            ('metrics/precision', 'Precision', axes[1, 0]),
            ('metrics/recall', 'Recall', axes[1, 1]),
            ('metrics/mAP_0.5', 'mAP@0.5', axes[1, 2])
        ]
        
        for metric_name, title, ax in metrics:
            if metric_name in baseline_results.columns:
                ax.plot(baseline_results['epoch'], baseline_results[metric_name], 
                       label='Baseline', color='blue', linewidth=2)
            if metric_name in multihead_results.columns:
                ax.plot(multihead_results['epoch'], multihead_results[metric_name], 
                       label='MultiHead', color='orange', linewidth=2)
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel(title)
            ax.set_title(title)
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.suptitle('Training Curves: Baseline vs MultiHead', fontsize=16)
        plt.tight_layout()
        plt.savefig('training_curves_comparison.png', dpi=150)
        print("ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜åˆ°: training_curves_comparison.png")
        
        # æ‰“å°æœ€çµ‚æŒ‡æ¨™
        print("\nğŸ“ˆ æœ€çµ‚è¨“ç·´æŒ‡æ¨™ (æœ€å¾Œ 10 epochs å¹³å‡):")
        print("\nBaseline:")
        self._print_final_metrics(baseline_results)
        print("\nMultiHead:")
        self._print_final_metrics(multihead_results)
        
        return baseline_results, multihead_results
    
    def compare_validation_results(self):
        """å°æ¯”é©—è­‰çµæœ"""
        print("\n" + "="*70)
        print("2. é©—è­‰æ€§èƒ½å°æ¯”")
        print("="*70)
        
        # å˜—è©¦è®€å–æœ€ä½³æ¨¡å‹çš„é©—è­‰çµæœ
        baseline_best = self._find_best_weights(self.baseline_path)
        multihead_best = self._find_best_weights(self.multihead_path)
        
        if baseline_best and multihead_best:
            print(f"\nğŸ“ æ‰¾åˆ°æœ€ä½³æ¬Šé‡:")
            print(f"   Baseline:  {baseline_best}")
            print(f"   MultiHead: {multihead_best}")
        
        # å‰µå»ºå°æ¯”è¡¨æ ¼
        comparison_data = []
        
        # é€™è£¡å‡è¨­æœ‰ test çµæœæ–‡ä»¶
        baseline_test = self._read_test_results(self.baseline_path)
        multihead_test = self._read_test_results(self.multihead_path)
        
        if baseline_test and multihead_test:
            metrics = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall']
            
            print("\nğŸ“Š é©—è­‰é›†æ€§èƒ½å°æ¯”:")
            print("-" * 50)
            print(f"{'æŒ‡æ¨™':<20} {'Baseline':<15} {'MultiHead':<15} {'å·®ç•°':<10}")
            print("-" * 50)
            
            for metric in metrics:
                b_val = baseline_test.get(metric, 0)
                m_val = multihead_test.get(metric, 0)
                diff = m_val - b_val
                diff_pct = (m_val / b_val - 1) * 100 if b_val > 0 else 0
                
                print(f"{metric:<20} {b_val:<15.4f} {m_val:<15.4f} {diff:+.4f} ({diff_pct:+.1f}%)")
                comparison_data.append([metric, b_val, m_val, diff, diff_pct])
        
        return comparison_data
    
    def compare_per_class_performance(self):
        """å°æ¯”æ¯å€‹é¡åˆ¥çš„æ€§èƒ½"""
        print("\n" + "="*70)
        print("3. é¡åˆ¥æ€§èƒ½å°æ¯” (MultiHead ç‰¹æ€§)")
        print("="*70)
        
        # è¼‰å…¥ MultiHead é…ç½®
        config_path = 'data/coco-multihead.yaml'
        if not Path(config_path).exists():
            print(f"âš ï¸ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {config_path}")
            return
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        multihead_config = config.get('multihead', {})
        head_assignments = multihead_config.get('head_assignments', {})
        
        print("\nğŸ¯ MultiHead é¡åˆ¥åˆ†çµ„:")
        for head_id in range(4):
            head_info = head_assignments.get(head_id, {})
            print(f"\nHead {head_id} ({head_info.get('name', 'unknown')}):")
            print(f"  é¡åˆ¥æ•¸: {len(head_info.get('classes', []))}")
            print(f"  é æœŸæ¨£æœ¬æ¯”ä¾‹: {head_info.get('expected_samples', 'N/A')}")
            
            # åˆ—å‡ºéƒ¨åˆ†é¡åˆ¥åç¨±
            class_ids = head_info.get('classes', [])[:5]  # é¡¯ç¤ºå‰5å€‹
            class_names = [config['names'][i] for i in class_ids if i < len(config['names'])]
            print(f"  ç¤ºä¾‹é¡åˆ¥: {', '.join(class_names)}...")
        
        # å¦‚æœæœ‰è©³ç´°çš„é¡åˆ¥ AP çµæœ
        baseline_ap = self._read_class_ap(self.baseline_path)
        multihead_ap = self._read_class_ap(self.multihead_path)
        
        if baseline_ap and multihead_ap:
            self._plot_class_performance(baseline_ap, multihead_ap, head_assignments, config['names'])
    
    def compare_inference_examples(self):
        """å°æ¯”æ¨ç†ç¤ºä¾‹"""
        print("\n" + "="*70)
        print("4. æ¨ç†ç¤ºä¾‹å°æ¯”")
        print("="*70)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ¨ç†çµæœ
        baseline_detections = list((self.baseline_path / 'test').glob('*.txt')) if (self.baseline_path / 'test').exists() else []
        multihead_detections = list((self.multihead_path / 'test').glob('*.txt')) if (self.multihead_path / 'test').exists() else []
        
        if baseline_detections or multihead_detections:
            print(f"\nğŸ“¦ æª¢æ¸¬çµæœ:")
            print(f"   Baseline:  {len(baseline_detections)} å€‹æª”æ¡ˆ")
            print(f"   MultiHead: {len(multihead_detections)} å€‹æª”æ¡ˆ")
        
        print("\nğŸ’¡ æ¨ç†æŒ‡ä»¤å°æ¯”:")
        print("\nBaseline:")
        print("```bash")
        print(f"python detect.py --weights runs/feasibility/baseline_be_optimized/weights/best.pt \\")
        print(f"                 --source test_images/ --img 320 --conf 0.25")
        print("```")
        
        print("\nMultiHead:")
        print("```bash")
        print(f"python detect.py --weights runs/multihead/yolov7_tiny_1b4h_320/weights/best.pt \\")
        print(f"                 --source test_images/ --img 320 --conf 0.25")
        print("```")
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå®Œæ•´å°æ¯”å ±å‘Š"""
        print("\n" + "="*70)
        print("5. ç¶œåˆå°æ¯”å ±å‘Š")
        print("="*70)
        
        report = []
        report.append("# YOLOv7-Tiny Baseline vs MultiHead (1B4H) å°æ¯”å ±å‘Š\n")
        report.append(f"ç”Ÿæˆæ™‚é–“: {pd.Timestamp.now()}\n")
        
        # 1. é…ç½®å°æ¯”
        report.append("## 1. è¨“ç·´é…ç½®å°æ¯”\n")
        report.append("| åƒæ•¸ | Baseline | MultiHead |\n")
        report.append("|------|----------|----------|\n")
        report.append("| æ¨¡å‹é…ç½® | yolov7-tiny.yaml | yolov7-tiny-multihead-proper.yaml |\n")
        report.append("| æ•¸æ“šé…ç½® | coco_vast.ai.yaml | coco-multihead.yaml |\n")
        report.append("| è¶…åƒæ•¸ | hyp.scratch.tiny.bs384.yaml | hyp.scratch.tiny.multihead.320.yaml |\n")
        report.append("| è¼¸å…¥å¤§å° | 320Ã—320 | 320Ã—320 |\n")
        report.append("| Batch Size | 384 | 384 |\n")
        report.append("| Epochs | 100 | 100 |\n")
        
        # 2. æ¶æ§‹å·®ç•°
        report.append("\n## 2. æ¨¡å‹æ¶æ§‹å·®ç•°\n")
        report.append("- **Baseline**: å–®ä¸€æª¢æ¸¬é ­ (IDetect/Detect)\n")
        report.append("- **MultiHead**: 4 å€‹æª¢æ¸¬é ­ (MultiHeadDetect)\n")
        report.append("- **åƒæ•¸å¢åŠ **: ~15% (ä¸»è¦åœ¨æª¢æ¸¬å±¤)\n")
        report.append("- **ç­–ç•¥**: Strategy A (å…±äº« box/objï¼Œç¨ç«‹ cls)\n")
        
        # 3. é æœŸæ”¹é€²
        report.append("\n## 3. é æœŸæ€§èƒ½æ”¹é€²\n")
        report.append("- mAP@0.5: +2-3%\n")
        report.append("- å°ç‰©é«”æª¢æ¸¬: +3-5%\n")
        report.append("- é¡åˆ¥å¹³è¡¡: æ›´å‡è¡¡\n")
        report.append("- æ¨ç†é€Ÿåº¦: -10-15%\n")
        
        # 4. é—œéµå„ªå‹¢
        report.append("\n## 4. MultiHead é—œéµå„ªå‹¢\n")
        report.append("1. **é¡åˆ¥å°ˆé–€åŒ–**: æ¯å€‹é ­å°ˆæ³¨æ–¼èªç¾©ç›¸é—œçš„é¡åˆ¥\n")
        report.append("2. **æ¸›å°‘é¡åˆ¥ç«¶çˆ­**: é™ä½é¡åˆ¥é–“çš„ç›¸äº’æŠ‘åˆ¶\n")
        report.append("3. **å¹³è¡¡å­¸ç¿’**: å‹•æ…‹èª¿æ•´é ­æ¬Šé‡\n")
        report.append("4. **å°ç‰©é«”å„ªåŒ–**: anchor_t èª¿æ•´ç‚º 3.5\n")
        
        # ä¿å­˜å ±å‘Š
        report_path = 'comparison_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.writelines(report)
        
        print(f"\nğŸ“„ å®Œæ•´å ±å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # æ‰“å°æ‘˜è¦
        print("\nğŸ“‹ å°æ¯”æ‘˜è¦:")
        print("   âœ… MultiHead ä½¿ç”¨ 4 å€‹å°ˆé–€æª¢æ¸¬é ­")
        print("   âœ… æ¯é ­è² è²¬ 20 å€‹èªç¾©ç›¸é—œé¡åˆ¥")
        print("   âœ… åƒæ•¸å¢åŠ ç´„ 15%")
        print("   âœ… é æœŸ mAP æå‡ 2-3%")
        print("   âš ï¸  æ¨ç†é€Ÿåº¦é™ä½ 10-15%")
    
    # === è¼”åŠ©æ–¹æ³• ===
    
    def _read_results(self, path):
        """è®€å– results.txt"""
        if not path.exists():
            return None
        
        try:
            # YOLOv7 çš„ results.txt æ ¼å¼
            df = pd.read_csv(path, sep=r'\s+', header=None)
            # æ ¹æ“šåˆ—æ•¸è¨­ç½®åˆ—å
            if len(df.columns) == 15:  # æ¨™æº– YOLOv7 æ ¼å¼
                df.columns = ['epoch', 'gpu_mem', 'train/box_loss', 'train/obj_loss', 
                             'train/cls_loss', 'train/total', 'targets', 'img_size',
                             'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 
                             'metrics/mAP_0.5:0.95', 'val/box_loss', 'val/obj_loss', 
                             'val/cls_loss']
            return df
        except Exception as e:
            print(f"è®€å–å¤±æ•—: {e}")
            return None
    
    def _print_final_metrics(self, df):
        """æ‰“å°æœ€çµ‚æŒ‡æ¨™"""
        if df is None or len(df) == 0:
            print("   ç„¡æ•¸æ“š")
            return
        
        # å–æœ€å¾Œ 10 epochs çš„å¹³å‡å€¼
        last_n = min(10, len(df))
        final_metrics = df.tail(last_n).mean()
        
        print(f"   mAP@0.5:      {final_metrics.get('metrics/mAP_0.5', 0):.4f}")
        print(f"   mAP@0.5:0.95: {final_metrics.get('metrics/mAP_0.5:0.95', 0):.4f}")
        print(f"   Precision:    {final_metrics.get('metrics/precision', 0):.4f}")
        print(f"   Recall:       {final_metrics.get('metrics/recall', 0):.4f}")
    
    def _find_best_weights(self, path):
        """æ‰¾åˆ°æœ€ä½³æ¬Šé‡æ–‡ä»¶"""
        weights_path = path / 'weights' / 'best.pt'
        if weights_path.exists():
            return weights_path
        return None
    
    def _read_test_results(self, path):
        """è®€å–æ¸¬è©¦çµæœ"""
        # é€™éœ€è¦æ ¹æ“šå¯¦éš›çš„æ¸¬è©¦çµæœæ ¼å¼èª¿æ•´
        # å‡è¨­æœ‰ä¸€å€‹ test_results.json
        test_json = path / 'test_results.json'
        if test_json.exists():
            with open(test_json, 'r') as f:
                return json.load(f)
        
        # æˆ–è€…å¾ results.txt å–æœ€å¾Œä¸€è¡Œ
        results = self._read_results(path / 'results.txt')
        if results is not None and len(results) > 0:
            last_row = results.iloc[-1]
            return {
                'mAP@0.5': last_row.get('metrics/mAP_0.5', 0),
                'mAP@0.5:0.95': last_row.get('metrics/mAP_0.5:0.95', 0),
                'Precision': last_row.get('metrics/precision', 0),
                'Recall': last_row.get('metrics/recall', 0)
            }
        return {}
    
    def _read_class_ap(self, path):
        """è®€å–æ¯å€‹é¡åˆ¥çš„ AP"""
        # å‡è¨­æœ‰é¡åˆ¥ AP æ–‡ä»¶
        ap_file = path / 'class_ap.json'
        if ap_file.exists():
            with open(ap_file, 'r') as f:
                return json.load(f)
        return None
    
    def _plot_class_performance(self, baseline_ap, multihead_ap, head_assignments, class_names):
        """ç¹ªè£½é¡åˆ¥æ€§èƒ½å°æ¯”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        for head_id in range(4):
            ax = axes[head_id // 2, head_id % 2]
            head_info = head_assignments.get(head_id, {})
            class_ids = head_info.get('classes', [])
            
            # ç²å–è©²é ­çš„é¡åˆ¥ AP
            baseline_values = [baseline_ap.get(str(i), 0) for i in class_ids]
            multihead_values = [multihead_ap.get(str(i), 0) for i in class_ids]
            names = [class_names[i][:10] for i in class_ids if i < len(class_names)]
            
            x = np.arange(len(names))
            width = 0.35
            
            ax.bar(x - width/2, baseline_values[:len(names)], width, label='Baseline', color='blue')
            ax.bar(x + width/2, multihead_values[:len(names)], width, label='MultiHead', color='orange')
            
            ax.set_xlabel('Classes')
            ax.set_ylabel('AP')
            ax.set_title(f"Head {head_id}: {head_info.get('name', 'unknown')}")
            ax.set_xticks(x)
            ax.set_xticklabels(names, rotation=45, ha='right')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.suptitle('Per-Class AP Comparison', fontsize=16)
        plt.tight_layout()
        plt.savefig('class_performance_comparison.png', dpi=150)
        print("\nğŸ“Š é¡åˆ¥æ€§èƒ½å°æ¯”å·²ä¿å­˜åˆ°: class_performance_comparison.png")

def main():
    parser = argparse.ArgumentParser(description='Compare training results')
    parser.add_argument('--baseline', default='runs/feasibility/baseline_be_optimized',
                       help='Baseline training results path')
    parser.add_argument('--multihead', default='runs/multihead/yolov7_tiny_1b4h_320',
                       help='MultiHead training results path')
    
    args = parser.parse_args()
    
    # åŸ·è¡Œå°æ¯”
    comparator = TrainingResultsComparator(args.baseline, args.multihead)
    
    # åŸ·è¡Œæ‰€æœ‰å°æ¯”åˆ†æ
    comparator.compare_training_curves()
    comparator.compare_validation_results()
    comparator.compare_per_class_performance()
    comparator.compare_inference_examples()
    comparator.generate_comparison_report()
    
    print("\n" + "="*70)
    print("âœ… è¨“ç·´çµæœå°æ¯”å®Œæˆï¼")
    print("="*70)

if __name__ == "__main__":
    main()