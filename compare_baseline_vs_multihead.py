#!/usr/bin/env python3
"""
å®Œæ•´å°æ¯” YOLOv7-Tiny Baseline vs MultiHead (1B4H)
åŒ…å«æ¨¡å‹çµæ§‹ã€æ€§èƒ½æŒ‡æ¨™ã€æ¨ç†é€Ÿåº¦ç­‰å…¨æ–¹ä½æ¯”è¼ƒ
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from pathlib import Path
import time
import json
from tabulate import tabulate
import matplotlib.pyplot as plt
import seaborn as sns
from models.yolo import Model
import argparse
import yaml

class ModelComparator:
    """æ¨¡å‹å°æ¯”å™¨"""
    
    def __init__(self, baseline_cfg, multihead_cfg, img_size=320):
        """
        åˆå§‹åŒ–å°æ¯”å™¨
        
        Args:
            baseline_cfg: Baseline æ¨¡å‹é…ç½®è·¯å¾‘
            multihead_cfg: MultiHead æ¨¡å‹é…ç½®è·¯å¾‘
            img_size: è¼¸å…¥åœ–ç‰‡å¤§å°
        """
        self.img_size = img_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è¼‰å…¥æ¨¡å‹
        print("è¼‰å…¥æ¨¡å‹ä¸­...")
        self.baseline_model = Model(baseline_cfg).to(self.device)
        self.multihead_model = Model(multihead_cfg).to(self.device)
        
        # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
        self.baseline_model.eval()
        self.multihead_model.eval()
        
        self.results = {}
    
    def compare_architecture(self):
        """æ¯”è¼ƒæ¨¡å‹æ¶æ§‹"""
        print("\n" + "="*70)
        print("1. æ¨¡å‹æ¶æ§‹å°æ¯”")
        print("="*70)
        
        results = {}
        
        # 1.1 åƒæ•¸é‡æ¯”è¼ƒ
        baseline_params = sum(p.numel() for p in self.baseline_model.parameters())
        multihead_params = sum(p.numel() for p in self.multihead_model.parameters())
        
        results['parameters'] = {
            'baseline': baseline_params,
            'multihead': multihead_params,
            'difference': multihead_params - baseline_params,
            'increase_pct': (multihead_params / baseline_params - 1) * 100
        }
        
        print(f"\nğŸ“Š åƒæ•¸é‡å°æ¯”:")
        print(f"   Baseline:  {baseline_params:,} åƒæ•¸")
        print(f"   MultiHead: {multihead_params:,} åƒæ•¸")
        print(f"   å¢åŠ :      {multihead_params - baseline_params:,} ({results['parameters']['increase_pct']:.1f}%)")
        
        # 1.2 æª¢æ¸¬å±¤å°æ¯”
        baseline_det = self.baseline_model.model[-1]
        multihead_det = self.multihead_model.model[-1]
        
        print(f"\nğŸ” æª¢æ¸¬å±¤å°æ¯”:")
        print(f"   Baseline:  {type(baseline_det).__name__}")
        print(f"   MultiHead: {type(multihead_det).__name__}")
        
        if hasattr(multihead_det, 'n_heads'):
            print(f"   æª¢æ¸¬é ­æ•¸é‡: {multihead_det.n_heads}")
        
        # 1.3 å±¤ç´šçµæ§‹å°æ¯”
        baseline_layers = len(self.baseline_model.model)
        multihead_layers = len(self.multihead_model.model)
        
        print(f"\nğŸ“ ç¶²çµ¡æ·±åº¦:")
        print(f"   Baseline:  {baseline_layers} å±¤")
        print(f"   MultiHead: {multihead_layers} å±¤")
        
        # 1.4 æª¢æ¸¬å±¤åƒæ•¸è©³ç´°å°æ¯”
        baseline_det_params = sum(p.numel() for p in baseline_det.parameters())
        multihead_det_params = sum(p.numel() for p in multihead_det.parameters())
        
        results['detection_layer'] = {
            'baseline': baseline_det_params,
            'multihead': multihead_det_params,
            'increase_pct': (multihead_det_params / baseline_det_params - 1) * 100
        }
        
        print(f"\nğŸ¯ æª¢æ¸¬å±¤åƒæ•¸:")
        print(f"   Baseline:  {baseline_det_params:,}")
        print(f"   MultiHead: {multihead_det_params:,}")
        print(f"   å¢åŠ :      {(multihead_det_params / baseline_det_params - 1) * 100:.1f}%")
        
        self.results['architecture'] = results
        return results
    
    def compare_inference_speed(self, num_runs=100, batch_sizes=[1, 8, 16, 32]):
        """æ¯”è¼ƒæ¨ç†é€Ÿåº¦"""
        print("\n" + "="*70)
        print("2. æ¨ç†é€Ÿåº¦å°æ¯”")
        print("="*70)
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"\nâ±ï¸  Batch Size = {batch_size}:")
            
            # å‰µå»ºå‡è¼¸å…¥
            x = torch.randn(batch_size, 3, self.img_size, self.img_size).to(self.device)
            
            # Warmup
            for _ in range(10):
                with torch.no_grad():
                    _ = self.baseline_model(x)
                    _ = self.multihead_model(x)
            
            # Baseline æ¸¬è©¦
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start = time.time()
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = self.baseline_model(x)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            baseline_time = (time.time() - start) / num_runs
            baseline_fps = batch_size / baseline_time
            
            # MultiHead æ¸¬è©¦
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start = time.time()
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = self.multihead_model(x)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            multihead_time = (time.time() - start) / num_runs
            multihead_fps = batch_size / multihead_time
            
            results[f'batch_{batch_size}'] = {
                'baseline_ms': baseline_time * 1000,
                'multihead_ms': multihead_time * 1000,
                'baseline_fps': baseline_fps,
                'multihead_fps': multihead_fps,
                'slowdown_pct': (multihead_time / baseline_time - 1) * 100
            }
            
            print(f"   Baseline:  {baseline_time*1000:.2f} ms ({baseline_fps:.1f} FPS)")
            print(f"   MultiHead: {multihead_time*1000:.2f} ms ({multihead_fps:.1f} FPS)")
            print(f"   é€Ÿåº¦å·®ç•°:  {results[f'batch_{batch_size}']['slowdown_pct']:.1f}%")
        
        self.results['inference_speed'] = results
        return results
    
    def compare_memory_usage(self):
        """æ¯”è¼ƒè¨˜æ†¶é«”ä½¿ç”¨"""
        print("\n" + "="*70)
        print("3. è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”")
        print("="*70)
        
        results = {}
        
        # æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
        baseline_size = sum(p.numel() * p.element_size() for p in self.baseline_model.parameters()) / 1024 / 1024
        multihead_size = sum(p.numel() * p.element_size() for p in self.multihead_model.parameters()) / 1024 / 1024
        
        results['model_size_mb'] = {
            'baseline': baseline_size,
            'multihead': multihead_size,
            'increase_pct': (multihead_size / baseline_size - 1) * 100
        }
        
        print(f"\nğŸ’¾ æ¨¡å‹å¤§å°:")
        print(f"   Baseline:  {baseline_size:.2f} MB")
        print(f"   MultiHead: {multihead_size:.2f} MB")
        print(f"   å¢åŠ :      {(multihead_size / baseline_size - 1) * 100:.1f}%")
        
        if torch.cuda.is_available():
            # GPU è¨˜æ†¶é«”ä½¿ç”¨ï¼ˆè¨“ç·´æ™‚ï¼‰
            batch_size = 16
            x = torch.randn(batch_size, 3, self.img_size, self.img_size).to(self.device)
            
            # Baseline
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            self.baseline_model.train()
            _ = self.baseline_model(x)
            baseline_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
            
            # MultiHead
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            self.multihead_model.train()
            _ = self.multihead_model(x)
            multihead_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
            
            results['gpu_memory_mb'] = {
                'baseline': baseline_memory,
                'multihead': multihead_memory,
                'increase_pct': (multihead_memory / baseline_memory - 1) * 100
            }
            
            print(f"\nğŸ® GPU è¨˜æ†¶é«” (BS=16, è¨“ç·´æ¨¡å¼):")
            print(f"   Baseline:  {baseline_memory:.2f} MB")
            print(f"   MultiHead: {multihead_memory:.2f} MB")
            print(f"   å¢åŠ :      {(multihead_memory / baseline_memory - 1) * 100:.1f}%")
            
            # æ¢å¾©è©•ä¼°æ¨¡å¼
            self.baseline_model.eval()
            self.multihead_model.eval()
        
        self.results['memory'] = results
        return results
    
    def compare_output_dimensions(self):
        """æ¯”è¼ƒè¼¸å‡ºç¶­åº¦"""
        print("\n" + "="*70)
        print("4. è¼¸å‡ºç¶­åº¦å°æ¯”")
        print("="*70)
        
        batch_size = 2
        x = torch.randn(batch_size, 3, self.img_size, self.img_size).to(self.device)
        
        with torch.no_grad():
            # Baseline è¼¸å‡º
            self.baseline_model.eval()
            baseline_out, _ = self.baseline_model(x)
            
            # MultiHead è¼¸å‡º
            self.multihead_model.eval()
            multihead_out, _ = self.multihead_model(x)
        
        print(f"\nğŸ“ æ¨ç†æ¨¡å¼è¼¸å‡ºå½¢ç‹€:")
        print(f"   Baseline:  {baseline_out.shape}")
        print(f"   MultiHead: {multihead_out.shape}")
        
        # è¨ˆç®—éŒ¨æ¡†æ•¸é‡
        total_anchors = baseline_out.shape[1]
        print(f"\nğŸ“¦ ç¸½éŒ¨æ¡†æ•¸: {total_anchors:,}")
        
        # ä¸åŒå°ºåº¦çš„éŒ¨æ¡†åˆ†ä½ˆ
        strides = [8, 16, 32]
        print(f"\nğŸ”¢ éŒ¨æ¡†åˆ†ä½ˆ ({self.img_size}Ã—{self.img_size}):")
        for stride in strides:
            grid_size = self.img_size // stride
            anchors = grid_size * grid_size * 3
            print(f"   P{int(np.log2(stride))}/{stride}: {grid_size}Ã—{grid_size}Ã—3 = {anchors:,} éŒ¨æ¡†")
        
        return {
            'baseline_shape': list(baseline_out.shape),
            'multihead_shape': list(multihead_out.shape),
            'total_anchors': total_anchors
        }
    
    def generate_comparison_table(self):
        """ç”Ÿæˆå°æ¯”è¡¨æ ¼"""
        print("\n" + "="*70)
        print("5. ç¶œåˆå°æ¯”è¡¨æ ¼")
        print("="*70)
        
        # æº–å‚™è¡¨æ ¼æ•¸æ“š
        data = []
        
        # æ¶æ§‹å°æ¯”
        if 'architecture' in self.results:
            arch = self.results['architecture']
            data.append(['ç¸½åƒæ•¸é‡', 
                        f"{arch['parameters']['baseline']:,}", 
                        f"{arch['parameters']['multihead']:,}",
                        f"+{arch['parameters']['increase_pct']:.1f}%"])
            data.append(['æª¢æ¸¬å±¤åƒæ•¸', 
                        f"{arch['detection_layer']['baseline']:,}", 
                        f"{arch['detection_layer']['multihead']:,}",
                        f"+{arch['detection_layer']['increase_pct']:.1f}%"])
        
        # æ¨ç†é€Ÿåº¦å°æ¯” (BS=1)
        if 'inference_speed' in self.results and 'batch_1' in self.results['inference_speed']:
            speed = self.results['inference_speed']['batch_1']
            data.append(['æ¨ç†æ™‚é–“ (ms)', 
                        f"{speed['baseline_ms']:.2f}", 
                        f"{speed['multihead_ms']:.2f}",
                        f"+{speed['slowdown_pct']:.1f}%"])
            data.append(['FPS (BS=1)', 
                        f"{speed['baseline_fps']:.1f}", 
                        f"{speed['multihead_fps']:.1f}",
                        f"-{(1 - speed['multihead_fps']/speed['baseline_fps'])*100:.1f}%"])
        
        # è¨˜æ†¶é«”å°æ¯”
        if 'memory' in self.results:
            mem = self.results['memory']
            data.append(['æ¨¡å‹å¤§å° (MB)', 
                        f"{mem['model_size_mb']['baseline']:.2f}", 
                        f"{mem['model_size_mb']['multihead']:.2f}",
                        f"+{mem['model_size_mb']['increase_pct']:.1f}%"])
            if 'gpu_memory_mb' in mem:
                data.append(['GPUè¨˜æ†¶é«” (MB)', 
                            f"{mem['gpu_memory_mb']['baseline']:.2f}", 
                            f"{mem['gpu_memory_mb']['multihead']:.2f}",
                            f"+{mem['gpu_memory_mb']['increase_pct']:.1f}%"])
        
        # æ‰“å°è¡¨æ ¼
        headers = ['æŒ‡æ¨™', 'Baseline', 'MultiHead (1B4H)', 'å·®ç•°']
        print("\n" + tabulate(data, headers=headers, tablefmt='grid'))
        
        return data
    
    def save_results(self, output_path='comparison_results.json'):
        """ä¿å­˜å°æ¯”çµæœ"""
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_path}")
    
    def plot_comparison(self):
        """ç¹ªè£½å°æ¯”åœ–è¡¨"""
        if not self.results:
            print("ç„¡çµæœå¯ç¹ªè£½")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. åƒæ•¸é‡å°æ¯”
        if 'architecture' in self.results:
            ax = axes[0, 0]
            params = self.results['architecture']['parameters']
            models = ['Baseline', 'MultiHead']
            values = [params['baseline'], params['multihead']]
            bars = ax.bar(models, values, color=['blue', 'orange'])
            ax.set_ylabel('Parameters')
            ax.set_title('Model Parameters Comparison')
            ax.bar_label(bars, fmt='%d')
        
        # 2. æ¨ç†é€Ÿåº¦å°æ¯”
        if 'inference_speed' in self.results:
            ax = axes[0, 1]
            batch_sizes = []
            baseline_fps = []
            multihead_fps = []
            
            for key in sorted(self.results['inference_speed'].keys()):
                bs = int(key.split('_')[1])
                batch_sizes.append(bs)
                baseline_fps.append(self.results['inference_speed'][key]['baseline_fps'])
                multihead_fps.append(self.results['inference_speed'][key]['multihead_fps'])
            
            ax.plot(batch_sizes, baseline_fps, 'o-', label='Baseline', color='blue')
            ax.plot(batch_sizes, multihead_fps, 's-', label='MultiHead', color='orange')
            ax.set_xlabel('Batch Size')
            ax.set_ylabel('FPS')
            ax.set_title('Inference Speed Comparison')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # 3. è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”
        if 'memory' in self.results:
            ax = axes[1, 0]
            mem = self.results['memory']['model_size_mb']
            models = ['Baseline', 'MultiHead']
            values = [mem['baseline'], mem['multihead']]
            bars = ax.bar(models, values, color=['blue', 'orange'])
            ax.set_ylabel('Size (MB)')
            ax.set_title('Model Size Comparison')
            ax.bar_label(bars, fmt='%.2f')
        
        # 4. ç›¸å°æ€§èƒ½åœ–
        ax = axes[1, 1]
        metrics = []
        percentages = []
        
        if 'architecture' in self.results:
            metrics.append('Parameters')
            percentages.append(self.results['architecture']['parameters']['increase_pct'])
        
        if 'inference_speed' in self.results and 'batch_1' in self.results['inference_speed']:
            metrics.append('Inference Time')
            percentages.append(self.results['inference_speed']['batch_1']['slowdown_pct'])
        
        if 'memory' in self.results:
            metrics.append('Model Size')
            percentages.append(self.results['memory']['model_size_mb']['increase_pct'])
        
        if metrics:
            colors = ['red' if p > 0 else 'green' for p in percentages]
            bars = ax.barh(metrics, percentages, color=colors)
            ax.set_xlabel('Change (%)')
            ax.set_title('Relative Performance (MultiHead vs Baseline)')
            ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, val in zip(bars, percentages):
                ax.text(val, bar.get_y() + bar.get_height()/2, 
                       f'{val:+.1f}%', 
                       ha='left' if val > 0 else 'right',
                       va='center')
        
        plt.tight_layout()
        plt.savefig('comparison_plots.png', dpi=150)
        print("\nğŸ“Š å°æ¯”åœ–è¡¨å·²ä¿å­˜åˆ°: comparison_plots.png")
        plt.show()

def main():
    parser = argparse.ArgumentParser(description='Compare Baseline vs MultiHead YOLOv7-Tiny')
    parser.add_argument('--baseline-cfg', default='cfg/training/yolov7-tiny.yaml',
                       help='Baseline model config')
    parser.add_argument('--multihead-cfg', default='cfg/training/yolov7-tiny-multihead-proper.yaml',
                       help='MultiHead model config')
    parser.add_argument('--img-size', type=int, default=320,
                       help='Input image size')
    parser.add_argument('--batch-sizes', nargs='+', type=int, default=[1, 8, 16, 32],
                       help='Batch sizes for speed test')
    parser.add_argument('--num-runs', type=int, default=100,
                       help='Number of runs for speed test')
    parser.add_argument('--output', default='comparison_results.json',
                       help='Output JSON file')
    parser.add_argument('--plot', action='store_true',
                       help='Generate comparison plots')
    
    args = parser.parse_args()
    
    # åŸ·è¡Œå°æ¯”
    comparator = ModelComparator(
        baseline_cfg=args.baseline_cfg,
        multihead_cfg=args.multihead_cfg,
        img_size=args.img_size
    )
    
    # åŸ·è¡Œæ‰€æœ‰å°æ¯”
    comparator.compare_architecture()
    comparator.compare_inference_speed(
        num_runs=args.num_runs,
        batch_sizes=args.batch_sizes
    )
    comparator.compare_memory_usage()
    comparator.compare_output_dimensions()
    
    # ç”Ÿæˆè¡¨æ ¼
    comparator.generate_comparison_table()
    
    # ä¿å­˜çµæœ
    comparator.save_results(args.output)
    
    # ç¹ªè£½åœ–è¡¨
    if args.plot:
        comparator.plot_comparison()
    
    print("\n" + "="*70)
    print("âœ… å°æ¯”å®Œæˆï¼")
    print("="*70)

if __name__ == "__main__":
    main()