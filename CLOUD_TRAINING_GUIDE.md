# é›²ç«¯é›™ä¸»æ©Ÿè¨“ç·´æŒ‡å—

## ğŸš€ æ¶æ§‹é…ç½®

### ä¸»æ©Ÿ A - è¨“ç·´ Baseline
- **ä»»å‹™**: YOLOv7-Tiny Baseline
- **GPU**: 1x RTX 3090 æˆ–æ›´é«˜
- **RAM**: 32GB+
- **å„²å­˜**: 100GB+

### ä¸»æ©Ÿ B - è¨“ç·´ MultiHead
- **ä»»å‹™**: YOLOv7-Tiny MultiHead (1B4H)
- **GPU**: 1x RTX 3090 æˆ–æ›´é«˜
- **RAM**: 32GB+
- **å„²å­˜**: 100GB+

---

## ğŸ“‹ åŸ·è¡Œæ­¥é©Ÿ

### Step 1: å…©å°ä¸»æ©Ÿéƒ½ Clone å°ˆæ¡ˆ

```bash
# åœ¨å…©å°ä¸»æ©Ÿä¸Šéƒ½åŸ·è¡Œ
git clone https://github.com/jimmychintw/yolov7_1B4H.git
cd yolov7_1B4H

# å®‰è£ä¾è³´
pip install -r requirements.txt
```

### Step 2: æº–å‚™ COCO æ•¸æ“šé›†

```bash
# å…©å°ä¸»æ©Ÿéƒ½éœ€è¦ä¸‹è¼‰ COCO
# é¸é … 1: ä½¿ç”¨è…³æœ¬ä¸‹è¼‰
bash scripts/get_coco.sh

# é¸é … 2: å¾æ‚¨çš„é›²ç«¯å„²å­˜è¤‡è£½
# gsutil cp -r gs://your-bucket/coco ./
# æˆ–
# aws s3 sync s3://your-bucket/coco ./coco
```

### Step 3: é…ç½®æ•¸æ“šè·¯å¾‘

**ä¸»æ©Ÿ A & B éƒ½è¦ä¿®æ”¹**ï¼š

ç·¨è¼¯ `data/coco.yaml` å’Œ `data/coco-multihead.yaml`ï¼š
```yaml
# ä¿®æ”¹ç‚ºå¯¦éš›è·¯å¾‘
train: /path/to/coco/train2017.txt
val: /path/to/coco/val2017.txt
test: /path/to/coco/test2017.txt
```

---

## ğŸ–¥ï¸ ä¸»æ©Ÿ A - Baseline è¨“ç·´

### å•Ÿå‹•è¨“ç·´è…³æœ¬

```bash
# å‰µå»ºè¨“ç·´è…³æœ¬
cat > train_baseline_cloud.sh << 'EOF'
#!/bin/bash

# YOLOv7-Tiny Baseline Training on Cloud
echo "Starting Baseline Training on $(hostname)"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)"
echo "Time: $(date)"

python train.py \
  --img-size 320 320 \
  --batch-size 384 \
  --epochs 100 \
  --data data/coco.yaml \
  --cfg cfg/training/yolov7-tiny.yaml \
  --weights '' \
  --hyp data/hyp.scratch.tiny.yaml \
  --device 0 \
  --workers 8 \
  --save_period 25 \
  --project runs/baseline \
  --name yolov7_tiny_baseline_320 \
  --exist-ok \
  --noautoanchor \
  --cache-images

echo "Training completed at $(date)"
EOF

chmod +x train_baseline_cloud.sh

# ä½¿ç”¨ nohup èƒŒæ™¯åŸ·è¡Œ
nohup ./train_baseline_cloud.sh > baseline_training.log 2>&1 &

# æˆ–ä½¿ç”¨ screen/tmux
screen -S baseline
./train_baseline_cloud.sh
# Ctrl+A+D åˆ†é›¢
```

### ç›£æ§è¨“ç·´

```bash
# æŸ¥çœ‹å³æ™‚æ—¥èªŒ
tail -f baseline_training.log

# æŸ¥çœ‹ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹ TensorBoard
tensorboard --logdir runs/baseline --host 0.0.0.0 --port 6006
```

---

## ğŸ–¥ï¸ ä¸»æ©Ÿ B - MultiHead è¨“ç·´

### å•Ÿå‹•è¨“ç·´è…³æœ¬

```bash
# å‰µå»ºè¨“ç·´è…³æœ¬
cat > train_multihead_cloud.sh << 'EOF'
#!/bin/bash

# YOLOv7-Tiny MultiHead (1B4H) Training on Cloud
echo "Starting MultiHead Training on $(hostname)"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)"
echo "Time: $(date)"

python train.py \
  --img-size 320 320 \
  --batch-size 384 \
  --epochs 100 \
  --data data/coco-multihead.yaml \
  --cfg cfg/training/yolov7-tiny-multihead-proper.yaml \
  --weights '' \
  --hyp data/hyp.scratch.tiny.multihead.320.yaml \
  --device 0 \
  --workers 8 \
  --save_period 25 \
  --project runs/multihead \
  --name yolov7_tiny_1b4h_320 \
  --exist-ok \
  --noautoanchor \
  --cache-images

echo "Training completed at $(date)"
EOF

chmod +x train_multihead_cloud.sh

# ä½¿ç”¨ nohup èƒŒæ™¯åŸ·è¡Œ
nohup ./train_multihead_cloud.sh > multihead_training.log 2>&1 &

# æˆ–ä½¿ç”¨ screen/tmux
screen -S multihead
./train_multihead_cloud.sh
# Ctrl+A+D åˆ†é›¢
```

### ç›£æ§è¨“ç·´

```bash
# æŸ¥çœ‹å³æ™‚æ—¥èªŒ
tail -f multihead_training.log

# æŸ¥çœ‹ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹ TensorBoard
tensorboard --logdir runs/multihead --host 0.0.0.0 --port 6006
```

---

## ğŸ“Š Step 4: æ”¶é›†çµæœ

### å…©å€‹è¨“ç·´éƒ½å®Œæˆå¾Œ

**ä¸»æ©Ÿ A - ä¸‹è¼‰ Baseline çµæœ**ï¼š
```bash
# æ‰“åŒ…çµæœ
tar -czf baseline_results.tar.gz runs/baseline/

# ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜
# Google Cloud
gsutil cp baseline_results.tar.gz gs://your-bucket/

# AWS
aws s3 cp baseline_results.tar.gz s3://your-bucket/

# æˆ–ç›´æ¥ SCP åˆ°æœ¬åœ°
scp baseline_results.tar.gz your_local_machine:/path/to/save/
```

**ä¸»æ©Ÿ B - ä¸‹è¼‰ MultiHead çµæœ**ï¼š
```bash
# æ‰“åŒ…çµæœ
tar -czf multihead_results.tar.gz runs/multihead/

# ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜
gsutil cp multihead_results.tar.gz gs://your-bucket/
aws s3 cp multihead_results.tar.gz s3://your-bucket/

# æˆ–ç›´æ¥ SCP åˆ°æœ¬åœ°
scp multihead_results.tar.gz your_local_machine:/path/to/save/
```

---

## ğŸ”„ Step 5: åˆä½µçµæœä¸¦å°æ¯”

### åœ¨æœ¬åœ°æˆ–ç¬¬ä¸‰å°æ©Ÿå™¨ä¸Š

```bash
# ä¸‹è¼‰å…©å€‹çµæœ
wget https://your-storage/baseline_results.tar.gz
wget https://your-storage/multihead_results.tar.gz

# è§£å£“
tar -xzf baseline_results.tar.gz
tar -xzf multihead_results.tar.gz

# Clone å°ˆæ¡ˆï¼ˆå¦‚æœé‚„æ²’æœ‰ï¼‰
git clone https://github.com/jimmychintw/yolov7_1B4H.git
cd yolov7_1B4H

# åŸ·è¡Œå°æ¯”
python compare_training_results.py \
    --baseline runs/baseline/yolov7_tiny_baseline_320 \
    --multihead runs/multihead/yolov7_tiny_1b4h_320

# ç”Ÿæˆå®Œæ•´å ±å‘Š
./run_comparison.sh
```

---

## ğŸ’° æˆæœ¬å„ªåŒ–å»ºè­°

### 1. ä½¿ç”¨ Spot/Preemptible å¯¦ä¾‹
```bash
# è¨­ç½®æª¢æŸ¥é»è‡ªå‹•ä¿å­˜
--save_period 10  # æ¯ 10 epochs ä¿å­˜

# æ”¯æ´æ–·é»çºŒè¨“
--resume runs/xxx/weights/last.pt
```

### 2. é¸æ“‡åˆé©çš„å¯¦ä¾‹

| é›²ç«¯å¹³å° | æ¨è–¦å¯¦ä¾‹ | GPU | æˆæœ¬ (ç´„) |
|---------|---------|-----|-----------|
| **AWS** | p3.2xlarge | V100 16GB | $3.06/hr |
| **GCP** | n1-standard-8 + V100 | V100 16GB | $2.48/hr |
| **Azure** | NC6s_v3 | V100 16GB | $3.06/hr |
| **Vast.ai** | RTX 3090 | RTX 3090 24GB | $0.5-1/hr |
| **Lambda Labs** | RTX 3090 | RTX 3090 24GB | $1.10/hr |

### 3. æ™‚é–“ä¼°ç®—

- **100 epochs @ batch 384**: ç´„ 3-4 å°æ™‚
- **ç¸½æˆæœ¬**: $6-24ï¼ˆå…©å°ä¸»æ©Ÿä¸¦è¡Œï¼‰

---

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: CUDA Out of Memory
```bash
# é™ä½ batch size
--batch-size 256  # æˆ– 128
```

### å•é¡Œ 2: è¨“ç·´ä¸­æ–·
```bash
# å¾æª¢æŸ¥é»æ¢å¾©
python train.py --resume runs/xxx/weights/last.pt ...
```

### å•é¡Œ 3: ç¶²è·¯å‚³è¼¸æ…¢
```bash
# ä½¿ç”¨é›²ç«¯å…§éƒ¨å‚³è¼¸
# ä¾‹å¦‚ï¼šå…©å°ä¸»æ©Ÿåœ¨åŒä¸€å€åŸŸï¼Œä½¿ç”¨å…§ç¶² IP
```

---

## ğŸ“ˆ å¯¦æ™‚ç›£æ§å„€è¡¨æ¿

### è¨­ç½®é ç«¯ TensorBoard

**ä¸»æ©Ÿ A**:
```bash
tensorboard --logdir runs/baseline --host 0.0.0.0 --port 6006
# é–‹æ”¾é˜²ç«ç‰† port 6006
```

**ä¸»æ©Ÿ B**:
```bash
tensorboard --logdir runs/multihead --host 0.0.0.0 --port 6007
# é–‹æ”¾é˜²ç«ç‰† port 6007
```

**æœ¬åœ°è¨ªå•**:
- http://ä¸»æ©ŸA-IP:6006 - Baseline
- http://ä¸»æ©ŸB-IP:6007 - MultiHead

### ä½¿ç”¨ Weights & Biases (å¯é¸)

```bash
# å®‰è£
pip install wandb

# ç™»å…¥
wandb login

# åœ¨è¨“ç·´æŒ‡ä»¤åŠ å…¥
--wandb
```

---

## âœ… å®Œæˆæª¢æŸ¥æ¸…å–®

- [ ] å…©å°ä¸»æ©Ÿéƒ½å·²å®‰è£ä¾è³´
- [ ] COCO æ•¸æ“šé›†å·²æº–å‚™
- [ ] æ•¸æ“šè·¯å¾‘å·²é…ç½®
- [ ] è¨“ç·´è…³æœ¬å·²å‰µå»º
- [ ] èƒŒæ™¯è¨“ç·´å·²å•Ÿå‹•
- [ ] TensorBoard ç›£æ§æ­£å¸¸
- [ ] å®šæœŸæª¢æŸ¥è¨“ç·´é€²åº¦
- [ ] çµæœå·²ä¸‹è¼‰å‚™ä»½
- [ ] å°æ¯”åˆ†æå·²å®Œæˆ

---

## ğŸ¯ é æœŸçµæœ

### è¨“ç·´æ™‚é–“ï¼ˆä¸¦è¡Œï¼‰
- **ç¸½æ™‚é–“**: 3-4 å°æ™‚ï¼ˆè€Œé 6-8 å°æ™‚ä¸²è¡Œï¼‰
- **æˆæœ¬**: ç´„ $12-24

### æ€§èƒ½å°æ¯”
- **Baseline mAP@0.5**: X%
- **MultiHead mAP@0.5**: X+2-3%
- **æ”¹é€²**: +2-3%

### è¼¸å‡ºæ–‡ä»¶
```
baseline_results/
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ best.pt
â”‚   â””â”€â”€ last.pt
â”œâ”€â”€ results.txt
â””â”€â”€ results.png

multihead_results/
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ best.pt
â”‚   â””â”€â”€ last.pt
â”œâ”€â”€ results.txt
â””â”€â”€ results.png

comparison_results/
â”œâ”€â”€ comparison_report.md
â”œâ”€â”€ training_curves_comparison.png
â””â”€â”€ comparison_results.json
```